## Abstract
|             Title | Adaptive Consistency Regularization for Semi-Supervised Transfer Learning |
| ----------------: | :----------------------------------------------------------- |
|       **Problem** | In this work, we consider semi-supervised learning and transfer learning jointly, leading to a more practical and competitive paradigm that can utilize both powerful pre-trained models from source domain as well as labeled/unlabeled data in the target domain.|
|    **Motivation** | 1. While recent studies on semi-supervised learning have shown remarkable progress in leveraging both labeled and unlabeled data, most of them presume a basic setting of the model is randomly initialized. <br> 2. The investigation of a systematic solution on DNN-based semi-supervised transfer learning has rarely been delved into.|
|       **Results** | 1. Proposed adaptive consistency regularization outperforms state-of-the-art semi-supervised learning techniques such as Pseudo Label, Mean Teacher, and FixMatch. <br> 2. Moreover, our algorithm is orthogonal to existing methods and thus able to gain additional improvements on top of MixMatch and FixMatch.|
|    **Conclusion** | 1. We propose two regularization methods: Adaptive Knowledge Consistency (AKC) between the source and target model and Adaptive Representation Consistency (ARC) between labeled and unlabeled examples <br> 2. We show that AKC and ARC are competitive among state-of-the-art SSL methods. <br> 3. Furthermore, by incorporating AKC and ARC with other SSL methods, we achieve the best performance among several baseline methods on various transfer learning benchmarks. <br> 4.  Additionally, our adaptive consistency regularization methods could be used for more general transfer learning and (semi-) supervised learning frameworks.|
| **Contributions** | 1. **The first to propose an advanced end-to-end semi-supervised transfer learning framework for deep neural networks**. Considering incorporating inductive transfer learning, our research is **closer to the actual problems in practice**. <br> 2. **Introduce adaptive consistency regularization** to improve semi-supervised transfer learning by exploiting the characteristics of both semi-supervised learning and transfer learning, including **cross-task knowledge distillation with adaptive sample importance named Adaptive Knowledge Consistency and representation adaptation** for supervised learning using selected unlabeled data as the reference named Adaptive Representation Consistency. <br> 3. Conduct extensive experiments and show that the proposed adaptive consistency regularization is superior to classic semi-supervised learning algorithms such as Pseudo Label, Mean Teacher, and MixMatch on various semi-supervised transfer learning tasks. Furthermore, our method is shown orthogonal to existing methods and can obtain additional improvements even on top of MixMatch and FixMatch, which combine several state-of-the-art SSL techniques. |
|           **URL** | [CVPR 2021](https://openaccess.thecvf.com/content/CVPR2021/html/Abuduweili_Adaptive_Consistency_Regularization_for_Semi-Supervised_Transfer_Learning_CVPR_2021_paper.html) |
|     **My Rating** | ★★★★☆                                                    |
|      **Comments** ||

## Overview
